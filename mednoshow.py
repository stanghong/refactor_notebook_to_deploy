# -*- coding: utf-8 -*-
"""MedNoShow_Improve_Cycle4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11l-Uqa8z0nwmyQ5VuoAWgfU1MXZNL8s3

## Predict No Show For Hospital Appointments 
### Data From IM Insurence
Cycle 2: Not happy with the results. Non of the features has high enough impact on our classification \
Let's try some different approach to see if we can make difference

###Steps to predict Hospital Noshow \
***step1:*** conda create -n carprediction python=3.8 \
***step2:*** activate carprediction \
   Cd /Users/hongtang/15_Mentoring_Videos/01_EndtoEnd_Project1 \
***step3:***ML modeling \
***step4:*** create requirement.txt in python \
    pip freeze >requirements.txt \
    Pip Install flask \
***step5:*** Run "myapp.py" in flask \
	1.Need template; html file for a simple template interface\
	2.Need pkl file from ML model \
***Step6:*** Predict function using the ML model\



Resources:https://medium.com/techcrush/how-to-deploy-your-ml-model-in-jupyter-notebook-to-your-flask-app-d1c4933b29b5
https://www.youtube.com/watch?v=p_tpQSY1aTs
https://stackoverflow.com/questions/48205495/python-how-to-run-multiple-flask-apps-from-same-client-machine

future work: power BI deployment

****Column,Description **** \
\
MemberID,Member ID \
AppointmentID,Appointment ID \
Gender,"M = Male, F = Female" \
ScheduledDay,Date the appointment was scheduled \
AppointmentDay,Actual appointment date \
Age,Age\
LocationID,Patient Geography ID\
MedicaidIND,"1 = Medicaid patient, 0 = Non-Medicaid patient"\
Hypertension,"Hypertension indicator 1 = Yes, 0 = No"\
Diabetes,"Diabetes indicator 1 = Yes, 0 = No"\
Alcoholism,"Alcoholism indicator 1 = Yes, 0 = No"\
Disability,"Disability indicator 1 = Yes, 0 = No"\
SMS_received,"Text was sent to patient as an appointment reminder 1 = Yes, 0 = Yes"\
No-show,"Yes = Did not attend the appointment, No = Appoinment was kept"\
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns



#mount google drive:https://stackoverflow.com/questions/48376580/google-colab-how-to-read-data-from-my-google-drive
from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('drive/MyDrive/2021_DS_Projects/DS_Project/Medical_No_Shows.csv')

df.shape

df.head()

##check missing values
df.isnull().sum()

df.describe()
df.columns
df.info()

print(df['Gender'].unique())
print(df['LocationID'].unique())
print(df['MedicaidIND'].unique())
print(df['Hypertension'].unique())
print(df['Diabetes'].unique())
print(df['Alcoholism'].unique())
print(df['Disability'].unique())
print(df['SMS_received'].unique())
print(df['No-show'].unique())

"""### Time Series"""

#convert time into timestamp
df.ScheduledDay=pd.to_datetime(df.ScheduledDay)
df.AppointmentDay=pd.to_datetime(df.AppointmentDay)

# time of the day
print(min(df.AppointmentDay), max(df.AppointmentDay))
print(min(df.ScheduledDay),max(df.ScheduledDay))

def Noshowind(x):
    if x=='No': return 0
    else: return 1

Appt_No_show=df['No-show'].apply(lambda x:Noshowind(x))
Appt_No_show.index = df['AppointmentDay']
Appt_No_show.plot()

# final["appt_wd"].value_counts()
# create feature of week day
final=pd.DataFrame()
Appt_weekday = df['AppointmentDay'].dt.day_name()
Scheduled_weekday=df['ScheduledDay'].dt.day_name()

#prepare countplot
final = pd.DataFrame(list(zip(Appt_weekday, Scheduled_weekday,Appt_No_show)), columns =['appt_wd', 'schedule_wd','Noshow'])
final['no_show']=df[['No-show']]

final

ax=sns.countplot(x="appt_wd", order=['Monday','Tuesday','Wednesday','Thursday','Friday'], data=final,palette='rainbow')
for p, label in zip(ax.patches, final["appt_wd"].value_counts()):
     ax.annotate(label, (p.get_x()+0.2, p.get_height()+0.15))

finalns=final[final['no_show']=='No']

ax=sns.countplot(x="schedule_wd", order=['Monday','Tuesday','Wednesday','Thursday','Friday'], data=finalns,palette='rainbow')
for p, label in zip(ax.patches, finalns["schedule_wd"].value_counts()):
     ax.annotate(label, (p.get_x()+0.2, p.get_height()+0.15))

"""####  Appt and Schedule on Monday -Wednesday tend to have higher no show no"""

final.describe()

# difference of the appointment-schedule day
ScheduledDay=pd.to_datetime(df.ScheduledDay)
AppointmentDay=pd.to_datetime(df.AppointmentDay)
df['time_diff_days']=abs(AppointmentDay-ScheduledDay).dt.days

final['ApptDaysAfterSchedule']=df[['time_diff_days']]
ax=sns.violinplot(x='no_show', y='ApptDaysAfterSchedule', data=final,palette='rainbow')
ax.set(ylim=(0, 75))

"""### EDA Analysis"""

final_dataset=df[['PatientID','AppointmentID', 'Gender', 'ScheduledDay',
       'AppointmentDay', 'Age', 'LocationID', 'MedicaidIND', 'Hypertension',
       'Diabetes', 'Alcoholism', 'Disability', 'SMS_received', 'No-show',
       'time_diff_days']]

plt.figure(figsize=(5,2))
sns.countplot(x='Gender',hue='No-show',data=final_dataset,palette='Set1')

plt.figure(figsize=(5,2))
sns.countplot(x='Disability',hue='No-show',data=final_dataset,palette='Set1')

final_dataset.head()

"""### EDA and Feature Engineering"""

def ConvNoshow(x):
    if x=='Yes': return 1
    else: return 0
final_dataset['NoShow']=final_dataset['No-show'].apply(ConvNoshow)
final_dataset.drop(['No-show'],axis=1,inplace=True)

def ConvGender(x):
    if x=='M': return 1
    else: return 0
final_dataset['GenderC']=final_dataset['Gender'].apply(ConvGender)
final_dataset.drop(['Gender'],axis=1,inplace=True)

final_dataset.head()

final_dataset.info()

final_dataset.head()

# final_dataset['appt_wd']=final['appt_wd']
# final_dataset['schedule_wd']=final['schedule_wd']
final_dataset['appt_wd']= df['AppointmentDay'].dt.weekday
final_dataset['schedule_wd']=df['ScheduledDay'].dt.weekday
final_dataset['PatientIDLength']=df['PatientID'].apply(lambda x:len(x))

final_dataset.info()

final=final_dataset.select_dtypes('int')
final

"""### EDA Feature Selection"""

first_column = final.pop('NoShow')
# insert column using insert(position,column_name,first_column) function
final.insert(0, 'NoShow', first_column)

final.corr()

final

import seaborn as sns

# sns.pairplot(final_dataset)
corrmat=final.corr()
top_corr_features = corrmat.index

#get correlations of each features in dataset
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(final_dataset.corr(),annot=True,cmap="RdYlGn")

X=final.iloc[:,1:]
y=final.iloc[:,:1]

X.head()

y.head()

"""### Split Training and Testing sets"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)

y_train.value_counts()

"""### Feature Importance Evaluation and Selection"""

final.info()

### Logistic Regression

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

"""** Train and fit a logistic regression model on the training set.**

###Evaluate feature importance from information gain

https://github.com/krishnaik06/Complete-Feature-Selection/blob/master/3-%20Information%20gain%20-%20mutual%20information%20In%20Classification.ipynb
"""

from sklearn.feature_selection import mutual_info_classif

X_train

# determine the mutual information
mutual_info = mutual_info_classif(X_train, y_train.values.ravel())
mutual_info

mutual_info = pd.Series(mutual_info)
mutual_info.index = X_train.columns
mutual_info.sort_values(ascending=False)
mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 4))

from sklearn.feature_selection import SelectKBest

sel_five_cols = SelectKBest(mutual_info_classif, k=7)
sel_five_cols.fit(X_train, y_train.values.ravel())
X_train.columns[sel_five_cols.get_support()]

"""#### Conclustion: Comparing with Cycle 1; the new features could improve the prediction. It looks promising... """

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn import model_selection


# random forest model creation
# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74
    
rfc = RandomForestClassifier()
rfc.fit(X_train,y_train.values.ravel())
# predictions
rfc_predict = rfc.predict(X_test)

from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix

rfc_cv_score = cross_val_score(rfc, X_train,y_train.values.ravel(), cv=10, scoring='roc_auc')


print("=== Confusion Matrix ===")
print(confusion_matrix(y_test, rfc_predict))
print('\n')
print("=== Classification Report ===")
print(classification_report(y_test, rfc_predict))
print('\n')
print("=== All AUC Scores ===")
print(rfc_cv_score)
print('\n')
print("=== Mean AUC Score ===")
print("Mean AUC Score - Random Forest: ", rfc_cv_score.mean())

#https://blog.dataiku.com/narrowing-the-search-which-hyperparameters-really-matter
# https://github.com/codebasics/py/blob/master/ML/15_gridsearch/Exercise/15_grid_search_cv_exercise.ipynb

"""### Tuning HyperParameters for RandomForest"""

from IPython.display import Javascript
display(Javascript('IPython.notebook.execute_cells_below()'))

from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier

model_params = {
#     'svm': {
#         'model': svm.SVC(gamma='auto'),
#         'params' : {
#             'C': [10],  #1,10,20
#             'kernel': ['rbf'] #'rbf','linear'
#         }  
#     },
    'random_forest': {
        'model': RandomForestClassifier(),
        'params' : {
        'n_estimators': [100], #
        # Number of features to consider at every split
        'max_features' : ['auto'],
        # Minimum number of samples required at each leaf node
        'min_samples_leaf' : [2]

        }
    },
    'logistic_regression' : {
        'model': LogisticRegression(solver='liblinear',multi_class='auto'),
        'params': {
            'C': [1,5,10]
        }
    },
    'naive_bayes_gaussian': {
        'model': GaussianNB(),
        'params': {}
    },
    'decision_tree': {
        'model': DecisionTreeClassifier(),
        'params': {
            'criterion': ['gini','entropy'],
            
        }
    }     
}

from sklearn.model_selection import GridSearchCV
import pandas as pd
scores = []

for model_name, mp in model_params.items():
    clf =  GridSearchCV(mp['model'], mp['params'], cv=3, return_train_score=True)
    clf.fit(X_train, y_train.values.ravel())
    scores.append({
        'model': model_name,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })
    
df = pd.DataFrame(scores,columns=['model','best_score','best_params'])
df

# # clf.best_estimator_
# #  'best_index_',
# clf.best_params_
# #  'best_score_',

rfcmodel = RandomForestClassifier(n_estimators=100, criterion='gini', min_samples_split=5, 
                                  min_samples_leaf=2, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=42)
rfcmodel.fit(X_train,y_train.values.ravel())
y_pred_test = rfcmodel.predict(X_test)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_pred_test))
from sklearn import metrics
print(metrics.accuracy_score(y_test, y_pred_test))

rfc_cv_score = cross_val_score(rfcmodel, X_train,y_train.values.ravel(), cv=5, scoring='roc_auc')
print("=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred_test))
print('\n')
print("=== Classification Report ===")
print(classification_report(y_test, y_pred_test))
print('\n')
print("=== All AUC Scores ===")
print(rfc_cv_score)
print('\n')
print("=== Mean AUC Score ===")
print("Mean AUC Score - Random Forest: ", rfc_cv_score.mean())

"""### prepare pickle dump file for app deployment"""

import pickle
# open a file, where you ant to store the data
file = open('random_forest_classification_model.pkl', 'wb')

# dump information to that file
pickle.dump(rfcmodel, file)



